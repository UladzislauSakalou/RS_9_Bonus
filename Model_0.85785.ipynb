{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0ff92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import category_encoders as ce\n",
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Classifier\n",
    "from heamy.pipeline import ModelsPipeline\n",
    "from sklearn.preprocessing import normalize\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1020acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE=False\n",
    "ID = 'Id'\n",
    "TARGET = 'Cover_Type'\n",
    "NFOLDS = 5\n",
    "SEED = 4\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4fdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': 400,\n",
    "    'criterion': 'entropy',\n",
    "    'random_state': 4\n",
    "}\n",
    "\n",
    "rf1_params = {\n",
    "    'n_estimators': 400,\n",
    "    'criterion': 'gini',\n",
    "    'random_state': 4\n",
    "}\n",
    "\n",
    "et1_params = {\n",
    "    'n_estimators': 400,\n",
    "    'criterion': 'gini',\n",
    "    'random_state': 4\n",
    "}\n",
    "\n",
    "et_params = {\n",
    "    'n_estimators': 500,\n",
    "    'criterion': 'entropy',\n",
    "    'random_state': 4\n",
    "}\n",
    "\n",
    "et2_params = {\n",
    "    'n_estimators': 600,\n",
    "    'criterion': 'gini',\n",
    "    'random_state': 4\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'n_estimators': 700, \n",
    "    'learning_rate':0.1\n",
    "}\n",
    "\n",
    "rf2_params = {\n",
    "    'n_estimators': 200,\n",
    "    'criterion': 'entropy',\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "rf3_params = {\n",
    "    'n_estimators': 200,\n",
    "    'criterion': 'gini',\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "et3_params = {\n",
    "    'n_estimators': 200,\n",
    "    'criterion': 'gini',\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "et4_params = {\n",
    "    'n_estimators': 200,\n",
    "    'criterion': 'entropy',\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "lgb1_params = {\n",
    "    'n_estimators': 200, \n",
    "    'learning_rate':0.1\n",
    "}\n",
    "\n",
    "logr_params = {\n",
    "        'solver' : 'liblinear',\n",
    "        'multi_class' : 'ovr',\n",
    "        'C': 1,\n",
    "        'random_state': 0\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate': 0.05,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 7,        \n",
    "    'max_depth': 6,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "}\n",
    "\n",
    "xg_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.1,\n",
    "        'objective': 'multi:softprob',   \n",
    "        'num_class': 7,\n",
    "        'max_depth': 4,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'nrounds': 200\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a13907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(predictions):\n",
    "    submission = pd.read_csv('sampleSubmission.csv')\n",
    "    submission['Cover_Type'] = predictions\n",
    "    submission.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3753787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_first(X_train, y_train, X_test, y_test=None):\n",
    "    X_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    model = xgb.train(xg_params, X_train, xg_params['nrounds'])\n",
    "    return model.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fb6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELU_CODE = {\n",
    "    1:2702,2:2703,3:2704,4:2705,5:2706,6:2717,7:3501,8:3502,9:4201,\n",
    "    10:4703,11:4704,12:4744,13:4758,14:5101,15:5151,16:6101,17:6102,\n",
    "    18:6731,19:7101,20:7102,21:7103,22:7201,23:7202,24:7700,25:7701,\n",
    "    26:7702,27:7709,28:7710,29:7745,30:7746,31:7755,32:7756,33:7757,\n",
    "    34:7790,35:8703,36:8707,37:8708,38:8771,39:8772,40:8776\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b881f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatic_zone(input_df):\n",
    "    df = input_df.copy()\n",
    "    df['Climatic_Zone'] = input_df['Soil_Type'].apply(\n",
    "        lambda x: int(str(ELU_CODE[x])[0])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75bfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pca_features(train, test):\n",
    "    train_temp = train[['Horizontal_Distance_To_Roadways' , 'Elevation', 'Horizontal_Distance_To_Fire_Points']]\n",
    "    test_temp = test[['Horizontal_Distance_To_Roadways' , 'Elevation', 'Horizontal_Distance_To_Fire_Points']]\n",
    "    train_temp_scaled = (train_temp - train_temp.mean(axis=0)) / train_temp.std(axis=0)\n",
    "    test_temp_scaled = (test_temp - train_temp.mean(axis=0)) / train_temp.std(axis=0)\n",
    "    pca = PCA(n_components=2)\n",
    "    train_temp_pca = pca.fit_transform(train_temp_scaled)\n",
    "    test_temp_pca = pca.transform(test_temp_scaled)\n",
    "    train_temp_pca = pd.DataFrame(train_temp_pca, columns=['Roads_Fire_Elevation_pca_1', 'Roads_Fire_Elevation_pca_2'], index=train.index)\n",
    "    test_temp_pca = pd.DataFrame(test_temp_pca, columns=['Roads_Fire_Elevation_pca_1', 'Roads_Fire_Elevation_pca_2'], index=test.index)\n",
    "    train = pd.concat([train, train_temp_pca], axis=1)\n",
    "    test = pd.concat([test, test_temp_pca], axis=1)\n",
    "    \n",
    "    train_temp = train[['Elevation', 'Horizontal_Distance_To_Fire_Points']]\n",
    "    test_temp = test[['Elevation', 'Horizontal_Distance_To_Fire_Points']]\n",
    "    train_temp_scaled = (train_temp - train_temp.mean(axis=0)) / train_temp.std(axis=0)\n",
    "    test_temp_scaled = (test_temp - train_temp.mean(axis=0)) / train_temp.std(axis=0)\n",
    "    pca = PCA(n_components=2)\n",
    "    train_temp_pca = pca.fit_transform(train_temp_scaled)\n",
    "    test_temp_pca = pca.transform(test_temp_scaled)\n",
    "    train_temp_pca = pd.DataFrame(train_temp_pca, columns=['Fire_Elevation_pca_1', 'Fire_Elevation_pca_2'], index=train.index)\n",
    "    test_temp_pca = pd.DataFrame(test_temp_pca, columns=['Fire_Elevation_pca_1', 'Fire_Elevation_pca_2'], index=test.index)\n",
    "    train = pd.concat([train, train_temp_pca], axis=1)\n",
    "    test = pd.concat([test, test_temp_pca], axis=1)\n",
    "    \n",
    "    train_temp = train[['Horizontal_Distance_To_Roadways' , 'Horizontal_Distance_To_Fire_Points']]\n",
    "    test_temp = test[['Horizontal_Distance_To_Roadways' , 'Horizontal_Distance_To_Fire_Points']]\n",
    "    train_temp_scaled = (train_temp - train_temp.mean(axis=0)) / train_temp.std(axis=0)\n",
    "    test_temp_scaled = (test_temp - train_temp.mean(axis=0)) / train_temp.std(axis=0)\n",
    "    pca = PCA(n_components=2)\n",
    "    train_temp_pca = pca.fit_transform(train_temp_scaled)\n",
    "    test_temp_pca = pca.transform(test_temp_scaled)\n",
    "    train_temp_pca = pd.DataFrame(train_temp_pca, columns=['Roads_Fire_pca_1', 'Roads_Fire_pca_2'], index=train.index)\n",
    "    test_temp_pca = pd.DataFrame(test_temp_pca, columns=['Roads_Fire_pca_1', 'Roads_Fire_pca_2'], index=test.index)\n",
    "    train = pd.concat([train, train_temp_pca], axis=1)\n",
    "    test = pd.concat([test, test_temp_pca], axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04f2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1(df):\n",
    "    df['Ele_minus_VDtHyd'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n",
    "    df['Hydro_plus_Fire'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_minus_Fire'] = df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_plus_Road'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways']\n",
    "    df['Hydro_minus_Road'] = df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways']\n",
    "    df['Fire_plus_Road'] = df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways']\n",
    "    df['Fire_minus_Road'] = df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways']\n",
    "    df[\"Elevation_minus_Road_02\"] = df[\"Elevation\"] - .02*df[\"Horizontal_Distance_To_Roadways\"]\n",
    "    df[\"Elevation_hd\"] = df[\"Elevation\"] - .2*df[\"Horizontal_Distance_To_Hydrology\"]\n",
    "    df['Hillshade'] = df['Hillshade_9am'] + df['Hillshade_3pm'] + df['Hillshade_Noon']\n",
    "    df['Soil_Type'] = 0\n",
    "    for i in range(1,41):\n",
    "        df['Soil_Type'] += i * df[f'Soil_Type{i}']\n",
    "    df = climatic_zone(df)\n",
    "    df['Soil_Type'] = df['Soil_Type'].astype('str') \n",
    "    df['Vertical_Distance_To_Hydrology_sqrt'] = np.sqrt(np.abs(df['Vertical_Distance_To_Hydrology']))\n",
    "    df['Horizontal_Distance_To_Hydrology_sqrt'] = np.sqrt(np.abs(df['Horizontal_Distance_To_Hydrology']))\n",
    "    df['Horizontal_Distance_To_Fire_Points_log'] = np.log(1 + np.abs(df['Horizontal_Distance_To_Fire_Points']))\n",
    "    df['Horizontal_Distance_To_Roadways_log'] = np.log(1 + np.abs(df['Horizontal_Distance_To_Roadways']))\n",
    "    df['Wilderness_Area'] = 0\n",
    "    for i in range(1, 5):\n",
    "        df['Wilderness_Area'] += i * df[f'Wilderness_Area{i}']\n",
    "    \n",
    "    df['Wilderness_Area'] = df['Wilderness_Area'].astype('str') \n",
    "    \n",
    "    df.drop(['Hillshade_3pm'], axis=1, inplace=True)\n",
    "    df.drop(['Vertical_Distance_To_Hydrology'], axis=1, inplace=True)\n",
    "    df.drop(columns = [f'Soil_Type{i}' for i in range(1, 41)], inplace = True)\n",
    "    df.drop(columns = [f'Wilderness_Area{i}' for i in range(1, 5)], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8edb5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_enc_features(train, test):\n",
    "    enc = ce.OneHotEncoder().fit(train['Cover_Type'].astype(str))\n",
    "    y_onehot = enc.transform(train['Cover_Type'].astype(str))\n",
    "    class_names = y_onehot.columns\n",
    "    train_obj = train.select_dtypes('object')\n",
    "    test_obj = test.select_dtypes('object')\n",
    "    train = train.select_dtypes(exclude='object')\n",
    "    test = test.select_dtypes(exclude='object')\n",
    "    for class_ in class_names:\n",
    "        target_encoder = ce.TargetEncoder(smoothing=0)\n",
    "        target_encoder.fit(train_obj, y_onehot[class_])\n",
    "        temp_train = target_encoder.transform(train_obj)\n",
    "        temp_test = target_encoder.transform(test_obj)\n",
    "        temp_train.columns=[str(x)+'_'+str(class_) for x in temp_train.columns]\n",
    "        temp_test.columns = [str(x) + '_' + str(class_) for x in temp_test.columns]\n",
    "        train = pd.concat([train, temp_train], axis=1)\n",
    "        test = pd.concat([test, temp_test], axis=1)   \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4f9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset1():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    y_train = train[TARGET].ravel() - 1\n",
    "\n",
    "    train.drop([ID], axis=1, inplace=True)\n",
    "    test.drop([ID], axis=1, inplace=True)\n",
    "    \n",
    "    train = preprocess1(train)\n",
    "    test = preprocess1(test)\n",
    "    train, test = add_pca_features(train, test)\n",
    "    train, test = add_target_enc_features(train, test)    \n",
    "    train.drop([TARGET], axis=1, inplace=True)  \n",
    "\n",
    "    x_train = train.values\n",
    "    x_test = test.values\n",
    "\n",
    "    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731854f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(df):\n",
    "    df['Hydro_plus_Fire'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_minus_Fire'] = df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_plus_Road'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways']\n",
    "    df['Hydro_minus_Road'] = df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways']\n",
    "    df['Fire_plus_Road'] = df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways']\n",
    "    df['Fire_minus_Road'] = df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways']\n",
    "    df['Ele_plus_VDtHyd'] = df['Elevation'] + df['Vertical_Distance_To_Hydrology']\n",
    "    df['Ele_minus_VDtHyd'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n",
    "    df['Hydro_plus_Fire_avg'] = df['Hydro_plus_Fire'] / 2\n",
    "    df['Hydro_minus_Fire_avg'] = df['Hydro_minus_Fire'] / 2\n",
    "    df['Hydro_plus_Road_avg'] = df['Hydro_plus_Road'] / 2\n",
    "    df['Hydro_minus_Road_avg'] = df['Hydro_minus_Road'] / 2\n",
    "    df['Fire_plus_Road_avg'] = df['Fire_plus_Road'] / 2\n",
    "    df['Fire_minus_Road_avg'] = df['Fire_minus_Road'] / 2\n",
    "    df['Ele_plus_VDtHyd_avg'] = df['Ele_plus_VDtHyd'] / 2\n",
    "    df['Ele_minus_VDtHyd_avg'] = df['Ele_minus_VDtHyd'] / 2\n",
    "    df['slope_hyd_sqrt'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "    df['slope_hyd_sqrt'] = df['slope_hyd_sqrt'].map(lambda x: 0 if np.isinf(x) else x)\n",
    "    df['Amenities_avg'] = (df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways']) / 3 \n",
    "    df['Shadiness_morn_noon'] = df['Hillshade_9am'] / (df['Hillshade_Noon'] + 1)\n",
    "    df['Shadiness_noon_3pm'] = df['Hillshade_Noon'] / (df['Hillshade_3pm'] + 1)\n",
    "    df['Shadiness_morn_3'] = df['Hillshade_9am'] / (df['Hillshade_3pm'] + 1)\n",
    "    df['Shadiness_morn_avg'] = (df['Hillshade_9am'] + df['Hillshade_Noon']) / 2\n",
    "    df['Shadiness_afternoon'] = (df['Hillshade_Noon'] + df['Hillshade_3pm']) / 2\n",
    "    df['Shadiness_mean_hillshade'] =  (df['Hillshade_9am'] + df['Hillshade_Noon'] + df['Hillshade_3pm'] ) / 3    \n",
    "    df[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\n",
    "    df[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\n",
    "    df[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n",
    "    df[\"SlopeElevation\"] = df[\"Slope\"] * df[\"Elevation\"]\n",
    "    df[\"Vertical_Distance_To_Hydrology\"] = abs(df['Vertical_Distance_To_Hydrology'])\n",
    "    df['Neg_Elev_Hyd'] = df.Elevation-df.Horizontal_Distance_To_Hydrology*0.2\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6205d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset2():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    y_train = train[TARGET].ravel() - 1\n",
    "        \n",
    "    train.drop([ID, TARGET], axis=1, inplace=True)\n",
    "    test.drop([ID], axis=1, inplace=True)\n",
    "    \n",
    "    train = preprocess2(train)    \n",
    "    test = preprocess2(test)    \n",
    "    \n",
    "    cols_to_normalize = [ 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "                       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "                       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n",
    "                       'Horizontal_Distance_To_Fire_Points', \n",
    "                       'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n",
    "                       'Shadiness_morn_avg', \n",
    "                       'Shadiness_afternoon', \n",
    "                       'Shadiness_mean_hillshade',\n",
    "                       'Hydro_plus_Fire', 'Hydro_minus_Fire', \n",
    "                       'Hydro_plus_Road', 'Hydro_minus_Road', \n",
    "                       'Fire_plus_Road', 'Fire_minus_Road'\n",
    "                       ]\n",
    "\n",
    "    train[cols_to_normalize] = normalize(train[cols_to_normalize])\n",
    "    test[cols_to_normalize] = normalize(test[cols_to_normalize])   \n",
    "    \n",
    "    x_train = train.values\n",
    "    x_test = test.values\n",
    "\n",
    "    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(dataset1, dataset2):\n",
    "    rf = Classifier(dataset=dataset1, estimator = RandomForestClassifier, use_cache=CACHE, parameters=rf_params,name='rf')\n",
    "    et = Classifier(dataset=dataset1, estimator = ExtraTreesClassifier, use_cache=CACHE, parameters=et_params,name='et')   \n",
    "    rf1 = Classifier(dataset=dataset1, estimator=RandomForestClassifier, use_cache=CACHE, parameters=rf1_params,name='rf1')\n",
    "    et1 = Classifier(dataset=dataset1, use_cache=CACHE, estimator=ExtraTreesClassifier, parameters=et1_params,name='et1')\n",
    "    lgbc = Classifier(dataset=dataset1, estimator=LGBMClassifier, use_cache=CACHE, parameters=lgb_params,name='lgbc')\n",
    "    et2 = Classifier(dataset=dataset1,estimator=ExtraTreesClassifier, parameters=et2_params, use_cache=CACHE, name='et2')\n",
    "    xgb1 = Classifier(estimator=xgb_first, dataset=dataset1, use_cache=CACHE, name='xgb1')\n",
    "    \n",
    "    rf2 = Classifier(dataset=dataset2, estimator = RandomForestClassifier, use_cache=CACHE, parameters=rf2_params,name='rf2')\n",
    "    et3 = Classifier(dataset=dataset2, estimator = ExtraTreesClassifier, use_cache=CACHE, parameters=et3_params,name='et3')   \n",
    "    rf4 = Classifier(dataset=dataset2, estimator=RandomForestClassifier, use_cache=CACHE, parameters=rf3_params,name='rf4')\n",
    "    et4 = Classifier(dataset=dataset2, use_cache=CACHE, estimator=ExtraTreesClassifier, parameters=et4_params,name='et4')\n",
    "    lgbc2 = Classifier(dataset=dataset2, estimator=LGBMClassifier, use_cache=CACHE, parameters=lgb1_params,name='lgbc2')\n",
    "    gnb = Classifier(dataset=dataset2, estimator=GaussianNB, use_cache=CACHE, name='gnb')\n",
    "    logr = Classifier(dataset=dataset2, estimator=LogisticRegression, use_cache=CACHE, parameters=logr_params,name='logr')\n",
    "    xgb2 = Classifier(estimator=xgb_first, dataset=dataset2, use_cache=CACHE, name='xgb2')\n",
    "    \n",
    "    return ModelsPipeline(rf, et, rf1, et1, lgbc, et2, xgb1), ModelsPipeline(rf2, et3, rf4, et4, lgbc2, gnb, logr, xgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ceaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = Dataset(preprocessor=preprocess_dataset1, use_cache=True)\n",
    "dataset2 = Dataset(preprocessor=preprocess_dataset2, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65aad632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1, pipeline2 = create_pipelines(dataset1, dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fcc1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:21:13] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:21:33] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:21:54] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:22:13] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:22:31] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:22:48] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:30:17] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:30:54] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:31:32] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:32:12] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:32:51] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:33:27] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"nrounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack_ds1 = pipeline1.stack(k=NFOLDS,seed=SEED)\n",
    "stack_ds2 = pipeline2.stack(k=NFOLDS,seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31797288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lvl2(stack_ds):\n",
    "    dtrain = xgb.DMatrix(stack_ds.X_train, label=stack_ds.y_train)\n",
    "    dtest = xgb.DMatrix(stack_ds.X_test)\n",
    "\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=1000, \n",
    "             nfold=NFOLDS, seed=SEED, stratified=True,\n",
    "             early_stopping_rounds=20, show_stdv=False)\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    model = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    return model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe3541ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpreds_proba1 = create_model_lvl2(stack_ds1)\n",
    "xpreds_proba2 = create_model_lvl2(stack_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68992e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpreds_proba_final = np.mean([xpreds_proba1, xpreds_proba2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52f6226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y = dataset1.X_train, dataset1.X_test, dataset1.y_train + 1\n",
    "\n",
    "X_train_1_2 = X_train[y <= 2]\n",
    "X_train_3_6 = X_train[(y==3) | (y==6)]\n",
    "\n",
    "y_1_2 = y[y <= 2]\n",
    "y_3_6 = y[(y==3) | (y==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6768acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1_2 = ExtraTreesClassifier(n_estimators=200,n_jobs=-1,random_state=0)\n",
    "clf_1_2.fit(X_train_1_2, y_1_2)\n",
    "\n",
    "clf_3_6 = ExtraTreesClassifier(n_estimators=200,n_jobs=-1,random_state=0)\n",
    "clf_3_6.fit(X_train_3_6, y_3_6)\n",
    "\n",
    "preds_1_2 = clf_1_2.predict_proba(X_test)\n",
    "preds_3_6 = clf_3_6.predict_proba(X_test)\n",
    "preds = xpreds_proba_final\n",
    "\n",
    "preds[:, 0] += preds_1_2[:, 0] / 1.3\n",
    "preds[:, 1] += preds_1_2[:, 1] / 1.1\n",
    "preds[:, 2] += preds_3_6[:, 0] / 3.4\n",
    "preds[:, 5] += preds_3_6[:, 1] / 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa5ff240",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(preds, axis=1)).astype(int) + 1\n",
    "make_submission(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79012090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f369c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vlados] *",
   "language": "python",
   "name": "conda-env-vlados-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
